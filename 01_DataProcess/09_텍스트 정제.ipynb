{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 정제\n",
    "text_data = [\" 모듈은 정규 표현식을 사용하여 문자열에서 패턴을 찾고 조작하는 데 사용되는 파이썬 내장 모듈입니다. !!!!!\",\n",
    "                \"정규 표현식은 문자열 내에서 특정 패턴의 문자열을 찾거나 대체하는 데 유용합니다.\",\n",
    "                \"예를 들어, 이메일 주소나 전화번호 등과 같은 특정 패턴을 찾거나 HTML\",\n",
    "                \"태크를 제거하는 등의 작업을 수행할 때 유용합니다. ??!??\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['모듈은 정규 표현식을 사용하여 문자열에서 패턴을 찾고 조작하는 데 사용되는 파이썬 내장 모듈입니다. !!!!!', '정규 표현식은 문자열 내에서 특정 패턴의 문자열을 찾거나 대체하는 데 유용합니다.', '예를 들어, 이메일 주소나 전화번호 등과 같은 특정 패턴을 찾거나 HTML', '태크를 제거하는 등의 작업을 수행할 때 유용합니다. ??!??']\n"
     ]
    }
   ],
   "source": [
    "# 공백 문자 제거\n",
    "strip_whitespace = [string.strip() for string in text_data] # 공백 문자 제거\n",
    "\n",
    "print(strip_whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['모듈은 정규 표현식을 사용하여 문자열에서 패턴을 찾고 조작하는 데 사용되는 파이썬 내장 모듈입니다 !!!!!', '정규 표현식은 문자열 내에서 특정 패턴의 문자열을 찾거나 대체하는 데 유용합니다', '예를 들어, 이메일 주소나 전화번호 등과 같은 특정 패턴을 찾거나 HTML', '태크를 제거하는 등의 작업을 수행할 때 유용합니다 ??!??']\n"
     ]
    }
   ],
   "source": [
    "# 마침표 제거\n",
    "remove_periods = [string.replace(\".\",\"\") for string in strip_whitespace]\n",
    "print(remove_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 모듈은 정규 표현식을 사용하여 문자열에서 패턴을 찾고 조작하는 데 사용되는 파이썬 내장 모듈입니다 ', '정규 표현식은 문자열 내에서 특정 패턴의 문자열을 찾거나 대체하는 데 유용합니다', '예를 들어 이메일 주소나 전화번호 등과 같은 특정 패턴을 찾거나 HTML', '태크를 제거하는 등의 작업을 수행할 때 유용합니다 ']\n"
     ]
    }
   ],
   "source": [
    "# 구두점 삭제\n",
    "import sys\n",
    "import unicodedata\n",
    "\n",
    "temp = dict.fromkeys( i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n",
    "data = [string.translate(temp) for string in text_data]\n",
    "print(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 텍스트 토큰화\n",
    "- 텍스트를 개별 단어로 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/park.s.w/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# 구두점 데이터 다운로드\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['모듈은', '정규', '표현식을', '사용하여', '문자열에서', '패턴을', '찾고', '조작하는', '데', '사용되는', '파이썬', '내장', '모듈입니다', '.']\n"
     ]
    }
   ],
   "source": [
    "string = \"모듈은 정규 표현식을 사용하여 문자열에서 패턴을 찾고 조작하는 데 사용되는 파이썬 내장 모듈입니다.\"\n",
    "word_data = word_tokenize(string) # 단어를 토큰 기준으로 나눕니다.\n",
    "print(word_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['모듈은 정규 표현식을 사용하여 문자열에서 패턴을 찾고 조작하는 데 사용되는 파이썬 내장 모듈입니다.', '정규 표현식을 사용하여 문자열에서 패턴을 찾고 조작하는 데 사용되는 파이썬 내장 모듈입니다.']\n"
     ]
    }
   ],
   "source": [
    "sent_string = \"모듈은 정규 표현식을 사용하여 문자열에서 패턴을 찾고 조작하는 데 사용되는 파이썬 내장 모듈입니다. 정규 표현식을 사용하여 문자열에서 패턴을 찾고 조작하는 데 사용되는 파이썬 내장 모듈입니다.\"\n",
    "sent_data = sent_tokenize(sent_string)  # 문장 기준으로 나눕니다.\n",
    "print(sent_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불용어 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/park.s.w/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords') # 불용어 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we']\n",
      "불용어 리스트 :  ['i', 'me', 'my', 'myself', 'we']\n",
      "불용어가 삭제된 결과 :  ['going', 'go', 'store', 'park']\n"
     ]
    }
   ],
   "source": [
    "tokenized_words = [\"i\",\"am\",\"going\",\"to\",\"go\",\"the\",\"store\",\"and\",\"park\"]\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# 불용어 삭제\n",
    "filtered_words = [word for word in tokenized_words if word not in stop_words]\n",
    "\n",
    "# 불용어 확인\n",
    "stop_data = stop_words[:5]  # 불용어 확인\n",
    "print(stop_data)\n",
    "\n",
    "print(\"불용어 리스트 : \", stop_data)\n",
    "print(\"불용어가 삭제된 결과 : \", filtered_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 어간 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "am\n",
      "going\n",
      "to\n",
      "go\n",
      "the\n",
      "store\n",
      "and\n",
      "park\n",
      "['i', 'am', 'go', 'to', 'go', 'the', 'store', 'and', 'park']\n"
     ]
    }
   ],
   "source": [
    "tokenized_words_temp = [\"i\",\"am\",\"going\",\"to\",\"go\",\"the\",\"store\",\"and\",\"park\"]\n",
    "\n",
    "# 어간 추출기 생성\n",
    "porter = PorterStemmer()\n",
    "word_list_temp = []\n",
    "for word in tokenized_words :\n",
    "    print(word)\n",
    "    word_list_temp.append(porter.stem(word))\n",
    "    \n",
    "print(word_list_temp)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/park.s.w/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "text_data_tag = \"Chris loved outdoor running\"\n",
    "text_tagger =  pos_tag(word_tokenize(text_data_tag))\n",
    "print(text_tagger)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어 중요도에 가중치 부여하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.5773502691896257\n",
      "  (0, 2)\t0.5773502691896257\n",
      "  (0, 5)\t0.5773502691896257\n",
      "  (1, 0)\t0.5773502691896257\n",
      "  (1, 4)\t0.5773502691896257\n",
      "  (1, 6)\t0.5773502691896257\n",
      "  (2, 3)\t1.0\n",
      "{'love': 5, 'brazil': 2, 'braill': 1, 'sweden': 6, 'is': 4, 'best': 0, 'germany': 3}\n"
     ]
    }
   ],
   "source": [
    "text_data_01 = np.array(([\n",
    "    \"I Love Brazil. Braill I\",\n",
    "    \"Sweden is best\",\n",
    "    \"Germany\"\n",
    "]))\n",
    "\n",
    "# tf-idf 특성 행렬\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data_01)\n",
    "print(feature_matrix)\n",
    "\n",
    "# tf-idf 특성 행렬을 밀집 배열 확인\n",
    "feature_matrix.toarray()\n",
    "tf = tfidf.vocabulary_\n",
    "print(tf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Cloud 단어 뭉치를 가시화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.4.0 (SDL 2.26.4, Python 3.10.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pytagcloud\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'set' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tag \u001b[39m=\u001b[39m [{\u001b[39m'\u001b[39m\u001b[39mHello\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m100\u001b[39m}, {\u001b[39m'\u001b[39m\u001b[39mWorld\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m80\u001b[39m},{\u001b[39m'\u001b[39m\u001b[39mPython\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m200\u001b[39m}]\n\u001b[0;32m----> 2\u001b[0m tag_list \u001b[39m=\u001b[39m pytagcloud\u001b[39m.\u001b[39;49mmake_tags(tag, maxsize\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m) \u001b[39m# tag화 시켜줌\u001b[39;00m\n\u001b[1;32m      3\u001b[0m pytagcloud\u001b[39m.\u001b[39mcreate_tag_image(tag_list, \u001b[39m'\u001b[39m\u001b[39m../99_Studyfile/Output/word_cloud.jpg\u001b[39m\u001b[39m'\u001b[39m,size\u001b[39m=\u001b[39m(\u001b[39m900\u001b[39m,\u001b[39m600\u001b[39m), rectangular\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m webbrowser\u001b[39m.\u001b[39mopen(\u001b[39m'\u001b[39m\u001b[39mword_cloud.jpg\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytagcloud/__init__.py:119\u001b[0m, in \u001b[0;36mmake_tags\u001b[0;34m(wordcounts, minsize, maxsize, colors, scalef)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_tags\u001b[39m(wordcounts, minsize\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, maxsize\u001b[39m=\u001b[39m\u001b[39m36\u001b[39m, colors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, scalef\u001b[39m=\u001b[39mdefscale):\n\u001b[1;32m    111\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m    sizes and colors tags \u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m    wordcounts is a list of tuples(tags, count). (e.g. how often the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m    color is either chosen from colors (list of rgb tuples) if provided or random\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     counts \u001b[39m=\u001b[39m [tag[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m wordcounts]\n\u001b[1;32m    121\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(counts):\n\u001b[1;32m    122\u001b[0m         \u001b[39mreturn\u001b[39;00m []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytagcloud/__init__.py:119\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_tags\u001b[39m(wordcounts, minsize\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, maxsize\u001b[39m=\u001b[39m\u001b[39m36\u001b[39m, colors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, scalef\u001b[39m=\u001b[39mdefscale):\n\u001b[1;32m    111\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m    sizes and colors tags \u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m    wordcounts is a list of tuples(tags, count). (e.g. how often the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m    color is either chosen from colors (list of rgb tuples) if provided or random\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     counts \u001b[39m=\u001b[39m [tag[\u001b[39m1\u001b[39;49m] \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m wordcounts]\n\u001b[1;32m    121\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(counts):\n\u001b[1;32m    122\u001b[0m         \u001b[39mreturn\u001b[39;00m []\n",
      "\u001b[0;31mTypeError\u001b[0m: 'set' object is not subscriptable"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "tag = [{'Hello', 100}, {'World',80},{'Python',200}]\n",
    "tag_list = pytagcloud.make_tags(tag, maxsize=50) # tag화 시켜줌\n",
    "pytagcloud.create_tag_image(tag_list, '../99_Studyfile/Output/word_cloud.jpg',size=(900,600), rectangular=False)\n",
    "\n",
    "webbrowser.open('word_cloud.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
